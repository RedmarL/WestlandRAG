#!/usr/bin/env python
# -*- coding: utf-8 -*-
"""
Ingest chunks â†’ OpenAI embeddings â†’ Qdrant, with Dutch keywords per chunk.
Also builds a de-duplicated keyword set for later API use.
Now includes more granular metadata (date, category, author/department) in Qdrant payload.
"""

import os, json, time, traceback
from openai import OpenAI
from keybert import KeyBERT
from qdrant_client import QdrantClient
from qdrant_client.http import models
from sentence_transformers import SentenceTransformer
from stop_words import get_stop_words
import tiktoken # For token counting if needed, though chunk.py now handles it
from tqdm import tqdm # ADD THIS LINE

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ helpers â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
def log(msg: str) -> None:
    print(f"[{time.strftime('%Y-%m-%d %H:%M:%S')}] {msg}")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ config â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
DATA_PATH        = "all_chunks.json"      # Assuming all_chunks.json is generated after chunk.py
KEYWORD_OUTPATH  = "data/keyword_set.json"
COLLECTION_NAME  = "westland-openai-embedding"
EMBED_MODEL      = "text-embedding-3-large"
EMBED_DIM        = 3072 # Dimension of text-embedding-3-large
KW_MODEL_NAME    = "sentence-transformers/paraphrase-multilingual-mpnet-base-v2"
BATCH_SIZE       = 128
TOP_KW           = 5
DUTCH_STOP       = get_stop_words("dutch")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ load data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log(f"Loading chunks from {DATA_PATH} â€¦")
if not os.path.exists(DATA_PATH):
    log(f"Error: {DATA_PATH} not found. Please ensure 'chunk.py' and 'allchunks.py' have run.")
    exit(1)

with open(DATA_PATH, encoding="utf-8") as f:
    all_chunks = json.load(f)
log(f"Loaded {len(all_chunks)} chunks.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ initialize â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
client_openai = OpenAI(api_key=os.getenv("OPENAI_API_KEY"))
qdrant = QdrantClient("localhost", port=6333)
kw_model = KeyBERT(KW_MODEL_NAME) # Using KeyBERT for keyword extraction

# Ensure the collection exists and has the correct vector config
log(f"Ensuring Qdrant collection '{COLLECTION_NAME}' existsâ€¦")
qdrant.recreate_collection(
    collection_name=COLLECTION_NAME,
    vectors_config=models.VectorParams(size=EMBED_DIM, distance=models.Distance.COSINE),
    # Add payload indexing for faster filtering if you plan to filter frequently
    # For example, to filter by category or date range
    # hnsw_config=models.HnswConfigDiff(on_disk_payload=True) # Optional, can be useful for large payloads
)
log("Collection ensured.")

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ ingest data â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
total_uploaded = 0
keyword_set = set()

log("Starting chunk embedding and ingestionâ€¦")
# Process chunks in batches
for i in tqdm(range(0, len(all_chunks), BATCH_SIZE), desc="Embedding batches"):
    batch = all_chunks[i : i + BATCH_SIZE]
    texts = [chunk["text"] for chunk in batch]
    ids = [idx for idx, chunk in enumerate(batch)] # Simple incrementing ID, ensure uniqueness if needed

    # Generate embeddings
    t_embed_start = time.time()
    try:
        response = client_openai.embeddings.create(input=texts, model=EMBED_MODEL)
        vectors = [embedding.embedding for embedding in response.data]
    except Exception as e:
        log(f"Embedding failed on batch {i // BATCH_SIZE}: {e}")
        traceback.print_exc()
        continue
    log(f"Embedded {len(batch)} chunks in {time.time() - t_embed_start:.2f}s")

    points = []
    t_kw_start = time.time()
    for j, chunk in enumerate(batch):
        text = chunk["text"]
        idx = chunk["id"] # Use the UUID generated by chunk.py as the Qdrant ID

        # Extract keywords
        try:
            kws = [
                kw for kw, _ in kw_model.extract_keywords(
                    text,
                    stop_words=DUTCH_STOP,
                    top_n=TOP_KW,
                    use_mmr=True,
                    diversity=0.7,
                )
            ]
        except Exception as e:
            log(f"Keyword extraction failed on chunk {idx}: {e}")
            traceback.print_exc()
            kws = []

        keyword_set.update(kws)

        # Log a sample chunk's keywords for verification
        if j == 0:
            log(f"Sample chunk id={idx} â†’ kws={kws}")

        # Construct payload with all relevant metadata
        payload = {
            "text": text,
            "source": chunk["source_url"],          # 'source_url' from chunk.py
            "keywords": kws,
            "document_title": chunk.get("document_title"),
            "document_date": chunk.get("document_date"), # NEW: Date from chunk.py
            "category": chunk.get("category"),           # NEW: Category from chunk.py
            "author_department": chunk.get("author_department"), # NEW: Author/Department from chunk.py
            "document_type": chunk.get("document_type", "unknown") # Type, e.g., 'html_page'
        }

        points.append(
            models.PointStruct(
                id=idx, # Use the UUID from the chunk as the Qdrant point ID
                vector=vectors[j],
                payload=payload,
            )
        )

    log(f"Keywords for {len(batch)} chunks extracted in {time.time() - t_kw_start:.2f}s")

    t_upsert = time.time()
    qdrant.upsert(collection_name=COLLECTION_NAME, points=points)
    log(f"Upserted {len(points)} points in {time.time() - t_upsert:.2f}s")
    total_uploaded += len(points)

# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ after ingest â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
log(f"\nðŸŽ‰ Done! {total_uploaded} chunks embedded and stored in Qdrant.")
log(f"Saving {len(keyword_set)} unique keywords â†’ {KEYWORD_OUTPATH}")
os.makedirs(os.path.dirname(KEYWORD_OUTPATH), exist_ok=True)
with open(KEYWORD_OUTPATH, "w", encoding="utf-8") as f:
    json.dump(list(keyword_set), f, ensure_ascii=False, indent=2)
log("All keywords saved.")